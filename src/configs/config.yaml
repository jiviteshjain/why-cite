---
dataloaders:
  base_path: "./data/"
  
  shared_task_3c:
    special_tokens: ['#AUTHOR_TAG', '#author_tag']

  acl_arc:
    special_tokens: ['@CITATION', '@citation']
    context: "normal"  # extended/normal
    section_in_target: False
  
  scicite:
    section_in_target: True

losses:
  weighted_cross_entropy:
    weights: [0.30435841, 1.34843581, 2.91375291, 7.57575758, 1.78062678, 1.06837607]

  multi_task_weighted_cross_entropy:
    num_tasks: 2
    importance: [1, 0.08]
    weights:
      - [1.96, 25, 5.263, 20, 5.56, 25]
      # - [0.30435841, 1.34843581, 2.91375291, 7.57575758, 1.78062678, 1.06837607]
      - [3.20303605313093, 6.298507462686567, 15.486238532110091, 7.033333333333333, 21.367088607594937, 3.6301075268817202]

models:
  maheshwari_et_al:
    max_length: 512  # This is a characteristic of the model.
    dropout_probability: 0
    pretrained_identifier: "allenai/scibert_scivocab_uncased"

  two_task_transformer:
    max_length: 512
    intent_dropout_probability: 0
    section_dropout_probability: 0
    pretrained_identifier: "allenai/scibert_scivocab_uncased"

training:

  out_base_path: "out/"  # The run name will be auto-appended.
  
  run_name: "test_single_task"  # Keep this meaningful.
  trained_by: "jivitesh"
  trained_on: "joey"
  force_cold_start: True

  runner_in_use: "runner"
  model_in_use: "maheshwari_et_al"
  dataset_in_use: "acl_arc"
  loss_in_use: "weighted_cross_entropy"

  wandb:
    use: True
    entity: "why-cite"  # Do not change.
    project: "ours"  # Do not change.

  save_weights: False

  num_epochs: 5
  learning_rate: 1e-5
  lr_decay_factor: 1e-1  # Set to 1 to turn off.
  lr_decay_epochs: [3]
  weight_decay: 0.01  # Set to 0 to turn off.
  train_batch_size: 4
  val_batch_size: 4
  num_workers: 0
  train_shuffle: True
  val_shuffle: False

inference:
  model_path: "out/script-test/"  # Make sure to include the run name.

hydra:
  run:
    dir: hydra/${training.run_name}
